{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Azjou/Online-LoRA-Trainer/blob/main/Online_LoRA_Trainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"在Colab中运行\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![浏览人数](https://visitor-badge.glitch.me/badge?page_id=Azjou.Online_LoRA_Trainer)\n",
        "\n",
        "# Kohya LoRA Dreambooth<br><small><small>A Colab Notebook For LoRA Training (Dreambooth Method)\n"
      ],
      "metadata": {
        "id": "slgjeYgd6pWp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adapted to Google Colab based on [kohya-ss/sd-script](https://github.com/kohya-ss/sd-scripts)<br>\n",
        "Adapted to Google Colab by [Linaqruf](https://github.com/Linaqruf)<br>\n",
        "You can find latest notebook update [here](https://github.com/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-dreambooth.ipynb)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gPgBR3KM6E-Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Notebook Name | Description | Link |\n",
        "| --- | --- | --- |\n",
        "| [Kohya LoRA Dreambooth](https://github.com/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-dreambooth.ipynb) | LoRA Training (Dreambooth method) | [![](https://img.shields.io/static/v1?message=Open%20in%20Colab&logo=googlecolab&labelColor=5c5c5c&color=0f80c1&label=%20&style=for-the-badge)](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-dreambooth.ipynb) |\n",
        "| [Kohya LoRA Fine-Tuning](https://github.com/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-finetuner.ipynb) | LoRA Training (Fine-tune method) | [![](https://img.shields.io/static/v1?message=Open%20in%20Colab&logo=googlecolab&labelColor=5c5c5c&color=0f80c1&label=%20&style=for-the-badge)](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-finetuner.ipynb) |\n",
        "| [Kohya Trainer](https://github.com/Linaqruf/kohya-trainer/blob/main/kohya-trainer.ipynb) | Native Training | [![](https://img.shields.io/static/v1?message=Open%20in%20Colab&logo=googlecolab&labelColor=5c5c5c&color=0f80c1&label=%20&style=for-the-badge)](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-trainer.ipynb) |\n",
        "| [Kohya Dreambooth](https://github.com/Linaqruf/kohya-trainer/blob/main/kohya-dreambooth.ipynb) | Dreambooth Training | [![](https://img.shields.io/static/v1?message=Open%20in%20Colab&logo=googlecolab&labelColor=5c5c5c&color=0f80c1&label=%20&style=for-the-badge)](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-dreambooth.ipynb) |\n",
        "| Kohya Textual Inversion  | Textual Inversion Training | SOON |\n"
      ],
      "metadata": {
        "id": "D_6E01tRBfQU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTVqCAgSmie4"
      },
      "source": [
        "# I. Install Kohya Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_u3q60di584x",
        "cellView": "form",
        "outputId": "c6fb32d4-d5af-4bf8-a5dc-7cb9a6d37fe6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Apr  9 03:34:24 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   52C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Stored 'root_dir' (str)\n",
            "Stored 'repo_dir' (str)\n",
            "Stored 'tools_dir' (str)\n",
            "Stored 'finetune_dir' (str)\n",
            "Stored 'training_dir' (str)\n",
            "Cloning into '/content/kohya-trainer'...\n",
            "remote: Enumerating objects: 1874, done.\u001b[K\n",
            "remote: Counting objects: 100% (1874/1874), done.\u001b[K\n",
            "remote: Compressing objects: 100% (716/716), done.\u001b[K\n",
            "remote: Total 1874 (delta 1218), reused 1742 (delta 1156), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (1874/1874), 3.48 MiB | 20.71 MiB/s, done.\n",
            "Resolving deltas: 100% (1218/1218), done.\n"
          ]
        }
      ],
      "source": [
        "#@title ## 1.1. Clone Kohya Trainer\n",
        "#@markdown Clone Kohya Trainer from GitHub and check for updates. Use textbox below if you want to checkout other branch or old commit. Leave it empty to stay the HEAD on main.\n",
        "\n",
        "import os\n",
        "%store -r\n",
        "\n",
        "!nvidia-smi\n",
        "\n",
        "root_dir = \"/content\"\n",
        "%store root_dir\n",
        "repo_dir = str(root_dir)+\"/kohya-trainer\"\n",
        "%store repo_dir\n",
        "tools_dir = str(root_dir)+\"/kohya-trainer/tools\"\n",
        "%store tools_dir \n",
        "finetune_dir = str(root_dir)+\"/kohya-trainer/finetune\"\n",
        "%store finetune_dir\n",
        "training_dir = str(root_dir)+\"/dreambooth\"\n",
        "%store training_dir\n",
        "\n",
        "branch = \"\" #@param {type: \"string\"}\n",
        "repo_url = \"https://github.com/Linaqruf/kohya-trainer\"\n",
        "\n",
        "def clone_repo():\n",
        "  if os.path.isdir(repo_dir):\n",
        "    print(\"The repository folder already exists, will do a !git pull instead\\n\")\n",
        "    %cd {repo_dir}\n",
        "    !git pull origin {branch} if branch else !git pull\n",
        "  else:\n",
        "    %cd {root_dir}\n",
        "    !git clone {repo_url} {repo_dir}\n",
        "\n",
        "if not os.path.isdir(repo_dir):\n",
        "  clone_repo()\n",
        "\n",
        "%cd {root_dir}\n",
        "os.makedirs(repo_dir, exist_ok=True)\n",
        "os.makedirs(tools_dir, exist_ok=True)\n",
        "os.makedirs(finetune_dir, exist_ok=True)\n",
        "os.makedirs(training_dir, exist_ok=True)\n",
        "\n",
        "if branch:\n",
        "  %cd {repo_dir}\n",
        "  status = os.system(f\"git checkout {branch}\")\n",
        "  if status != 0:\n",
        "    raise Exception(\"Failed to checkout branch or commit\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNn0g1pnHfk5",
        "outputId": "d76626ce-8630-4445-aad5-8ae972b5cc6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/kohya-trainer\n",
            "Stored 'accelerate_config' (str)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m599.8/599.8 KB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 KB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 KB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.1/43.1 KB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.1/246.1 KB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.0/115.0 KB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for elfinder-client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following additional packages will be installed:\n",
            "  libaria2-0 libc-ares2 lz4\n",
            "The following NEW packages will be installed:\n",
            "  aria2 libaria2-0 libc-ares2 liblz4-tool lz4\n",
            "0 upgraded, 5 newly installed, 0 to remove and 24 not upgraded.\n",
            "Need to get 1,560 kB of archives.\n",
            "After this operation, 6,199 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libc-ares2 amd64 1.15.0-1ubuntu0.2 [36.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 libaria2-0 amd64 1.35.0-1build1 [1,082 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal/universe amd64 aria2 amd64 1.35.0-1build1 [356 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 lz4 amd64 1.9.2-2ubuntu0.20.04.1 [82.7 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 liblz4-tool all 1.9.2-2ubuntu0.20.04.1 [2,524 B]\n",
            "Fetched 1,560 kB in 3s (610 kB/s)\n",
            "Selecting previously unselected package libc-ares2:amd64.\n",
            "(Reading database ... 122349 files and directories currently installed.)\n",
            "Preparing to unpack .../libc-ares2_1.15.0-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking libc-ares2:amd64 (1.15.0-1ubuntu0.2) ...\n",
            "Selecting previously unselected package libaria2-0:amd64.\n",
            "Preparing to unpack .../libaria2-0_1.35.0-1build1_amd64.deb ...\n",
            "Unpacking libaria2-0:amd64 (1.35.0-1build1) ...\n",
            "Selecting previously unselected package aria2.\n",
            "Preparing to unpack .../aria2_1.35.0-1build1_amd64.deb ...\n",
            "Unpacking aria2 (1.35.0-1build1) ...\n",
            "Selecting previously unselected package lz4.\n",
            "Preparing to unpack .../lz4_1.9.2-2ubuntu0.20.04.1_amd64.deb ...\n",
            "Unpacking lz4 (1.9.2-2ubuntu0.20.04.1) ...\n",
            "Selecting previously unselected package liblz4-tool.\n",
            "Preparing to unpack .../liblz4-tool_1.9.2-2ubuntu0.20.04.1_all.deb ...\n",
            "Unpacking liblz4-tool (1.9.2-2ubuntu0.20.04.1) ...\n",
            "Setting up libc-ares2:amd64 (1.15.0-1ubuntu0.2) ...\n",
            "Setting up lz4 (1.9.2-2ubuntu0.20.04.1) ...\n",
            "Setting up liblz4-tool (1.9.2-2ubuntu0.20.04.1) ...\n",
            "Setting up libaria2-0:amd64 (1.35.0-1build1) ...\n",
            "Setting up aria2 (1.35.0-1build1) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.5/191.5 KB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m96.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 KB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 KB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 KB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m503.1/503.1 KB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.8/825.8 KB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 KB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m549.1/549.1 KB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.3/266.3 KB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m578.1/578.1 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m581.5/581.5 KB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m106.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 KB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 KB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 KB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for dadaptation (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for lycoris_lora (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for library (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.6/123.6 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#@title ## 1.2. Installing Dependencies\n",
        "#@markdown This will install required Python packages\n",
        "import os\n",
        "%store -r\n",
        "\n",
        "%cd {repo_dir}\n",
        "\n",
        "accelerate_config = os.path.join(repo_dir, \"accelerate_config/config.yaml\")\n",
        "%store accelerate_config\n",
        "install_xformers = True #@param {'type':'boolean'}\n",
        "\n",
        "def install_dependencies():\n",
        "    !pip -q install --upgrade gallery-dl gdown imjoy-elfinder\n",
        "    !apt -q install liblz4-tool aria2\n",
        "    !pip -q install --upgrade -r requirements.txt\n",
        "\n",
        "    if install_xformers:\n",
        "        !pip -q install -U -I --no-deps xformers==0.0.17rc481\n",
        "\n",
        "    from accelerate.utils import write_basic_config\n",
        "    if not os.path.exists(accelerate_config):\n",
        "        write_basic_config(save_location=accelerate_config)\n",
        "\n",
        "install_dependencies()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3. Sign-in to Cloud Service"
      ],
      "metadata": {
        "id": "qt9EJv5gQXuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ### 1.3.1. Login to Huggingface hub\n",
        "from huggingface_hub import login\n",
        "%store -r\n",
        "\n",
        "#@markdown Login to Huggingface hub\n",
        "#@markdown 1. You need a Huggingface account.\n",
        "#@markdown 2. To create a huggingface token, go to https://huggingface.co/settings/tokens, then create a new token or copy an available token with the `Write` role.\n",
        "write_token = \"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\" #@param {type:\"string\"}\n",
        "login(write_token, add_to_git_credential=True)\n",
        "\n",
        "%store write_token\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Rl2zERHbBQ9W",
        "outputId": "7aee6df0-9495-4d2f-eab6-ed200bce6396",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token is valid.\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n",
            "Stored 'write_token' (str)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ### 1.3.2. Mount Drive (Optional)\n",
        "from google.colab import drive\n",
        "\n",
        "mount_drive = True #@param {type: \"boolean\"}\n",
        "\n",
        "if mount_drive:\n",
        "  drive.mount('/content/drive')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "sKL38-WmQsLN",
        "outputId": "6e8703bf-22bb-4679-bca8-f05e90135530",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1.4. Open Special `File Explorer` for Colab (Optional)\n",
        "#@markdown This will work in real-time even while you run other cells.\n",
        "%store -r\n",
        "\n",
        "import threading\n",
        "from google.colab import output\n",
        "from imjoy_elfinder.app import main\n",
        "\n",
        "thread = threading.Thread(target=main, args=[[\"--root-dir=/content\", \"--port=8765\"]])\n",
        "thread.start()\n",
        "\n",
        "open_in_new_tab = True #@param {type:\"boolean\"}\n",
        "\n",
        "if open_in_new_tab:\n",
        "  output.serve_kernel_port_as_window(8765)\n",
        "else:\n",
        "  output.serve_kernel_port_as_iframe(8765, height='500')\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ZmIRAxgEQESm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gob9_OwTlwh"
      },
      "source": [
        "# II. Pretrained Model Selection"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 2.1. Download Available Model \n",
        "import os\n",
        "%store -r\n",
        "\n",
        "%cd {root_dir}\n",
        "\n",
        "installModels = []\n",
        "installv2Models = []\n",
        "\n",
        "#@markdown ### Available Model\n",
        "#@markdown Select one of available model to download:\n",
        "\n",
        "#@markdown ### SD1.x model\n",
        "modelUrl = [\"\", \\\n",
        "            \"https://huggingface.co/Linaqruf/personal-backup/resolve/main/models/animefull-final-pruned.ckpt\", \\\n",
        "            \"https://huggingface.co/cag/anything-v3-1/resolve/main/anything-v3-1.safetensors\", \\\n",
        "            \"https://huggingface.co/andite/anything-v4.0/resolve/main/anything-v4.5-pruned.ckpt\", \\\n",
        "            \"https://huggingface.co/Rasgeath/self_made_sauce/resolve/main/Kani-anime-pruned.ckpt\", \\\n",
        "            \"https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/Models/AbyssOrangeMix2/AbyssOrangeMix2_nsfw.safetensors\", \\\n",
        "            \"https://huggingface.co/gsdf/Counterfeit-V2.0/resolve/main/Counterfeit-V2.0fp16.safetensors\", \\\n",
        "            \"https://huggingface.co/closertodeath/dpepteahands3/resolve/main/dpepteahand3.ckpt\", \\\n",
        "            \"https://huggingface.co/prompthero/openjourney-v2/resolve/main/openjourney-v2.ckpt\", \\\n",
        "            \"https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0/resolve/main/dreamlike-diffusion-1.0.ckpt\", \\\n",
        "            \"https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.ckpt\"]\n",
        "modelList = [\"\", \\\n",
        "             \"Animefull-final-pruned\", \\\n",
        "             \"Anything-v3-1\", \\\n",
        "             \"Anything-v4-5-pruned\", \\\n",
        "             \"Kani-anime-pruned\", \\\n",
        "             \"AbyssOrangeMix2-nsfw\", \\\n",
        "             \"Counterfeit-v2\", \\\n",
        "             \"DpepTeaHands3\", \\\n",
        "             \"OpenJourney-v2\", \\\n",
        "             \"Dreamlike-diffusion-v1-0\", \\\n",
        "             \"Stable-Diffusion-v1-5\"]\n",
        "modelName = \"Anything-v3-1\"  #@param [\"\", \"Animefull-final-pruned\", \"Anything-v3-1\", \"Anything-v4-5-pruned\", \"Kani-anime-pruned\", \"AbyssOrangeMix2-nsfw\", \"Counterfeit-v2\", \"DpepTeaHands3\", \"OpenJourney-v2\", \"Dreamlike-diffusion-v1-0\", \"Stable-Diffusion-v1-5\"]\n",
        "\n",
        "#@markdown ### SD2.x model\n",
        "v2ModelUrl = [\"\", \\\n",
        "              \"https://huggingface.co/stabilityai/stable-diffusion-2-1-base/resolve/main/v2-1_512-ema-pruned.ckpt\", \\\n",
        "              \"https://huggingface.co/stabilityai/stable-diffusion-2-1/resolve/main/v2-1_768-ema-pruned.ckpt\", \\\n",
        "              \"https://huggingface.co/hakurei/waifu-diffusion-v1-4/resolve/main/wd-1-4-anime_e2.ckpt\", \\\n",
        "              \"https://huggingface.co/p1atdev/pd-archive/resolve/main/plat-v1-3-1.safetensors\"]\n",
        "v2ModelList = [\"\", \\\n",
        "              \"stable-diffusion-2-1-base\", \\\n",
        "              \"stable-diffusion-2-1-768v\", \\\n",
        "              \"waifu-diffusion-1-4-anime-e2\", \\\n",
        "              \"plat-diffusion-v1-3-1\"]\n",
        "v2ModelName = \"\" #@param [\"\", \"stable-diffusion-2-1-base\", \"stable-diffusion-2-1-768v\", \"waifu-diffusion-1-4-anime-e2\", \"plat-diffusion-v1-3-1\"]\n",
        "\n",
        "if modelName != \"\":\n",
        "  installModels.append((modelName, modelUrl[modelList.index(modelName)]))\n",
        "if v2ModelName != \"\":\n",
        "  installv2Models.append((v2ModelName, v2ModelUrl[v2ModelList.index(v2ModelName)]))\n",
        "\n",
        "def install(checkpoint_name, url):\n",
        "  ext = \"ckpt\" if url.endswith(\".ckpt\") else \"safetensors\"\n",
        "\n",
        "  hf_token = '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' \n",
        "  user_header = f\"\\\"Authorization: Bearer {hf_token}\\\"\"\n",
        "  !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -d {root_dir}/pre_trained_model -o {checkpoint_name}.{ext} \"{url}\"\n",
        "\n",
        "def install_checkpoint():\n",
        "  for model in installModels:\n",
        "    install(model[0], model[1])\n",
        "  for v2model in installv2Models:\n",
        "    install(v2model[0], v2model[1])\n",
        "\n",
        "install_checkpoint()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "wmnsZwClN1XL",
        "outputId": "8942e43d-9a8a-4845-a10b-448bf71a16dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            " *** Download Progress Summary as of Sat Apr  8 06:41:34 2023 *** \n",
            "=\n",
            "[#663f79 2.1GiB/3.9GiB(53%) CN:16 DL:197MiB ETA:9s]\n",
            "FILE: /content/pre_trained_model/Anything-v3-1.safetensors\n",
            "-\n",
            "\n",
            "\u001b[0m\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "663f79|\u001b[1;32mOK\u001b[0m  |   212MiB/s|/content/pre_trained_model/Anything-v3-1.safetensors\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 2.2. Download Custom Model\n",
        "\n",
        "import os\n",
        "%store -r\n",
        "\n",
        "%cd {root_dir}\n",
        "\n",
        "#@markdown ### Custom model\n",
        "modelUrl = \"https://huggingface.co/fangjiaqi/SDV1.5-pruned-emaonly-offical/resolve/main/v1-5-pruned-emaonly.ckpt\" #@param {'type': 'string'}\n",
        "dst = str(root_dir)+\"/pre_trained_model\"\n",
        "\n",
        "if not os.path.exists(dst):\n",
        "    os.makedirs(dst)\n",
        "\n",
        "def install(url):\n",
        "  base_name = os.path.basename(url)\n",
        "\n",
        "  if url.startswith(\"https://drive.google.com\"):\n",
        "    %cd {dst}\n",
        "    !gdown --fuzzy {url}\n",
        "  elif url.startswith(\"https://huggingface.co/\"):\n",
        "    if '/blob/' in url:\n",
        "      url = url.replace('/blob/', '/resolve/')\n",
        "    #@markdown Change this part with your own huggingface token if you need to download your private model\n",
        "    hf_token = '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' #@param {type:\"string\"}\n",
        "    user_header = f\"\\\"Authorization: Bearer {hf_token}\\\"\"\n",
        "    !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -d {dst} -o {base_name} {url}\n",
        "  else:\n",
        "    !aria2c --console-log-level=error --summary-interval=10 -c -x 16 -k 1M -s 16 -d {dst} -o {url}\n",
        "\n",
        "install(modelUrl)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "3LWn6GzNQ4j5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6546be24-baac-4d52-ab39-851602c23f10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            " *** Download Progress Summary as of Sat Apr  8 16:19:49 2023 *** \n",
            "=\n",
            "[#69e396 1.9GiB/3.9GiB(47%) CN:16 DL:210MiB ETA:10s]\n",
            "FILE: /content/pre_trained_model/v1-5-pruned-emaonly.ckpt\n",
            "-\n",
            "\n",
            " *** Download Progress Summary as of Sat Apr  8 16:20:00 2023 *** \n",
            "=\n",
            "[#69e396 3.8GiB/3.9GiB(96%) CN:16 DL:208MiB]\n",
            "FILE: /content/pre_trained_model/v1-5-pruned-emaonly.ckpt\n",
            "-\n",
            "\n",
            "\u001b[0m\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "69e396|\u001b[1;32mOK\u001b[0m  |   171MiB/s|/content/pre_trained_model/v1-5-pruned-emaonly.ckpt\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SoucgZQ6jgPQ",
        "outputId": "e0b48756-43d6-4d88-c5c4-e3712a23f5b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "\u001b[0m\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "997fff|\u001b[1;32mOK\u001b[0m  |   228MiB/s|/content/vae/stablediffusion.vae.pt\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n"
          ]
        }
      ],
      "source": [
        "#@title ## 2.3. Download Available VAE (Optional)\n",
        "%store -r \n",
        "\n",
        "%cd {root_dir}\n",
        "\n",
        "installVae = []\n",
        "#@markdown ### Available VAE\n",
        "#@markdown Select one of the VAEs to download, select `none` for not download VAE:\n",
        "vaeUrl = [\"\", \\\n",
        "          \"https://huggingface.co/Linaqruf/personal-backup/resolve/main/vae/animevae.pt\", \\\n",
        "          \"https://huggingface.co/hakurei/waifu-diffusion-v1-4/resolve/main/vae/kl-f8-anime.ckpt\", \\\n",
        "          \"https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt\"]\n",
        "vaeList = [\"none\", \\\n",
        "           \"anime.vae.pt\", \\\n",
        "           \"waifudiffusion.vae.pt\", \\\n",
        "           \"stablediffusion.vae.pt\"]\n",
        "vaeName = \"stablediffusion.vae.pt\" #@param [\"none\", \"anime.vae.pt\", \"waifudiffusion.vae.pt\", \"stablediffusion.vae.pt\"]\n",
        "\n",
        "installVae.append((vaeName, vaeUrl[vaeList.index(vaeName)]))\n",
        "\n",
        "def install(vae_name, url):\n",
        "  hf_token = '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'\n",
        "  user_header = f\"\\\"Authorization: Bearer {hf_token}\\\"\"\n",
        "  !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -o vae/{vae_name} \"{url}\"\n",
        "\n",
        "def install_vae():\n",
        "  if vaeName != \"none\":\n",
        "    for vae in installVae:\n",
        "      install(vae[0], vae[1])\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "install_vae()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "En9UUwGNMRMM"
      },
      "source": [
        "# III. Data Acquisition\n",
        "\n",
        "You have two options for acquiring your dataset: uploading it to this notebook or bulk downloading images from Danbooru using the image scraper. If you prefer to use your own dataset, simply upload it to Colab's local files.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 3.1. Define Train Data Directory\n",
        "#@markdown Define the location of your training data. This cell will also create a folder based on your input. Leave `reg_data_dir` empty for now.\n",
        "%store -r\n",
        "\n",
        "parent_directory = \"/content/dreambooth/train_data\" #@param {type: \"string\"}\n",
        "%store parent_directory\n",
        "reg_folder_directory = os.path.join(os.path.dirname(parent_directory), \"reg_data\")\n",
        "%store reg_folder_directory\n",
        "\n",
        "reg_repeats = 1 #@param {type: \"integer\"}\n",
        "train_repeats = 10 #@param {type: \"integer\"}\n",
        "concept_name = \"sksneurosama\" #@param {type: \"string\"}\n",
        "class_name = \"1girl\" #@param {type: \"string\"}\n",
        "#@markdown You can run this cell multiple time to add new concepts\n",
        "\n",
        "def get_folder_name(repeats, class_name, concept_name=None):\n",
        "  if class_name:\n",
        "    return f\"{repeats}_{concept_name} {class_name}\" if concept_name else f\"{repeats}_{class_name}\"\n",
        "  return f\"{repeats}_{concept_name}\"\n",
        "\n",
        "train_folder = get_folder_name(train_repeats, class_name, concept_name=concept_name)\n",
        "reg_folder = get_folder_name(reg_repeats, class_name)\n",
        "\n",
        "train_data_dir = os.path.join(parent_directory, train_folder)\n",
        "reg_data_dir = os.path.join(reg_folder_directory, reg_folder)\n",
        "\n",
        "os.makedirs(parent_directory, exist_ok=True)\n",
        "os.makedirs(reg_folder_directory, exist_ok=True)\n",
        "os.makedirs(train_data_dir, exist_ok=True)\n",
        "os.makedirs(reg_data_dir, exist_ok=True)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "kh7CeDqK4l3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 3.2. Download and Extract Zip (.zip)\n",
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "%store -r\n",
        "\n",
        "#@markdown ### Define Zipfile URL or Zipfile Path\n",
        "zipfile_url_or_path = \"https://huggingface.co/datasets/Linaqruf/your-dataset-name/resolve/main/masabodo_dataset.zip\" #@param {'type': 'string'}\n",
        "zipfile_dst = str(root_dir)+\"/zip_file.zip\"\n",
        "extract_to = \"\" #@param {'type': 'string'}\n",
        "\n",
        "if extract_to != \"\":\n",
        "  os.makedirs(extract_to, exist_ok=True)\n",
        "else:\n",
        "  extract_to = train_data_dir\n",
        "\n",
        "#@markdown This will ignore `extract_to` path and automatically extracting to `train_data_dir`\n",
        "is_dataset = True #@param{'type':'boolean'}\n",
        "\n",
        "#@markdown Tick this if you want to extract all files directly to `extract_to` folder, and automatically delete the zip to save the memory\n",
        "auto_unzip_and_delete = True #@param{'type':'boolean'}\n",
        "\n",
        "dirname = os.path.dirname(zipfile_dst)\n",
        "basename = os.path.basename(zipfile_dst)\n",
        "\n",
        "try:\n",
        "  if zipfile_url_or_path.startswith(\"/content\"):\n",
        "    zipfile_dst = zipfile_url_or_path\n",
        "    if auto_unzip_and_delete == False:\n",
        "      if is_dataset:\n",
        "        extract_to = train_data_dir\n",
        "      !unzip -j {zipfile_dst} -d \"{extract_to}\"\n",
        "  elif zipfile_url_or_path.startswith(\"https://drive.google.com\"):\n",
        "    !gdown --fuzzy  {zipfile_url_or_path}\n",
        "  elif zipfile_url_or_path.startswith(\"magnet:?\"):\n",
        "    !aria2c --summary-interval=10 -c -x 10 -k 1M -s 10 {zipfile_url_or_path}\n",
        "  elif zipfile_url_or_path.startswith(\"https://huggingface.co/\"):\n",
        "    if '/blob/' in zipfile_url_or_path:\n",
        "      zipfile_url_or_path = zipfile_url_or_path.replace('/blob/', '/resolve/')\n",
        "\n",
        "    hf_token = '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'\n",
        "    user_header = f\"\\\"Authorization: Bearer {hf_token}\\\"\"\n",
        "    !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -d {dirname} -o {basename} {zipfile_url_or_path}\n",
        "  else:\n",
        "    !aria2c --console-log-level=error --summary-interval=10 -c -x 16 -k 1M -s 16 -d {dirname} -o {basename} {zipfile_url_or_path}\n",
        "\n",
        "except Exception as e:\n",
        "  print(\"An error occurred while downloading the file:\", e)\n",
        "\n",
        "if is_dataset:\n",
        "  extract_to = train_data_dir\n",
        "\n",
        "if auto_unzip_and_delete:\n",
        "  !unzip -j {zipfile_dst} -d \"{extract_to}\"\n",
        "\n",
        "  files_to_move = (\"meta_cap.json\", \\\n",
        "                   \"meta_cap_dd.json\", \\\n",
        "                   \"meta_lat.json\", \\\n",
        "                   \"meta_clean.json\")\n",
        "\n",
        "  for filename in os.listdir(extract_to):\n",
        "      file_path = os.path.join(extract_to, filename)\n",
        "      if filename in files_to_move:\n",
        "          shutil.move(file_path, os.path.dirname(extract_to))\n",
        "  \n",
        "  path_obj = Path(zipfile_dst)\n",
        "  zipfile_name = path_obj.parts[-1]\n",
        "  \n",
        "  if os.path.isdir(zipfile_dst):\n",
        "    print(\"\\nThis zipfile doesn't exist or has been deleted \\n\")\n",
        "  else:\n",
        "    os.remove(zipfile_dst)\n",
        "    print(f\"\\n{zipfile_name} has been deleted\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "eFFHVTWNZGbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "A0t1dfnU5Xkq"
      },
      "outputs": [],
      "source": [
        "#@title ## 3.3. Simple Booru Scraper (Optional)\n",
        "#@markdown Use gallery-dl to scrape images from a booru site using the specified tags\n",
        "import os\n",
        "import html\n",
        "%store -r\n",
        "\n",
        "%cd {root_dir}\n",
        "\n",
        "booru = \"Gelbooru\" #@param [\"\", \"Danbooru\", \"Gelbooru\"]\n",
        "tag1 = \"cleo_(dragalia_lost) \" #@param {type: \"string\"}\n",
        "tag2 = \"\" #@param {type: \"string\"}\n",
        "download_tags = False #@param {type: \"boolean\"}\n",
        "\n",
        "if tag2 != \"\":\n",
        "  tags = tag1 + \"+\" + tag2\n",
        "else:\n",
        "  tags = tag1\n",
        "\n",
        "write_tags = \"--write-tags\" if download_tags == True else \"\"\n",
        "\n",
        "if booru.lower() == \"danbooru\":\n",
        "  !gallery-dl \"https://danbooru.donmai.us/posts?tags={tags}\" {write_tags} -D \"{train_data_dir}\"\n",
        "elif booru.lower() == \"gelbooru\":\n",
        "  !gallery-dl \"https://gelbooru.com/index.php?page=post&s=list&tags={tags}\" {write_tags} -D \"{train_data_dir}\"\n",
        "else:\n",
        "  print(f\"Unknown booru site: {booru}\")\n",
        "\n",
        "if download_tags == True: \n",
        "  files = [f for f in os.listdir(train_data_dir) if f.endswith(\".txt\")]\n",
        "\n",
        "  for file in files:\n",
        "      file_path = os.path.join(train_data_dir, file)\n",
        "\n",
        "      with open(file_path, \"r\") as f:\n",
        "          contents = f.read()\n",
        "\n",
        "      contents = html.unescape(contents)\n",
        "      contents = contents.replace(\"_\", \" \")\n",
        "      contents = \", \".join(contents.split(\"\\n\"))\n",
        "\n",
        "      with open(file_path, \"w\") as f:\n",
        "          f.write(contents)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IV. Data Preprocessing"
      ],
      "metadata": {
        "id": "zUiL2sLg7swG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 4.1. RGBA to RGB converter (Optional)\n",
        "\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import concurrent.futures\n",
        "from PIL import Image\n",
        "\n",
        "#@markdown Transparent images can negatively impact training results, resulting in good characters with bad backgrounds after the model is finished. To address this issue, this code will convert your transparent dataset with alpha channel (RGBA) to RGB and give it a white background. Alternatively, you can set random colors for a more diverse dataset. \n",
        "\n",
        "#@markdown `NOTE: All images endswith .webp will be deleted after the conversion process is completed.`\n",
        "\n",
        "random_color = False #@param {type:\"boolean\"}\n",
        "\n",
        "batch_size = 32 #@param {type:\"number\"}\n",
        "\n",
        "images = [image for image in os.listdir(train_data_dir) if image.endswith('.png') or image.endswith('.webp')]\n",
        "background_colors = [(255, 255, 255), \n",
        "                     (0, 0, 0), \n",
        "                     (255, 0, 0), \n",
        "                     (0, 255, 0), \n",
        "                     (0, 0, 255), \n",
        "                     (255, 255, 0), \n",
        "                     (255, 0, 255), \n",
        "                     (0, 255, 255)]\n",
        "\n",
        "def process_image(image_name):\n",
        "    img = Image.open(f'{train_data_dir}/{image_name}')\n",
        "\n",
        "    if img.mode in ('RGBA', 'LA'):\n",
        "        if random_color:\n",
        "          background_color = random.choice(background_colors)\n",
        "        else:\n",
        "          background_color = (255, 255, 255)\n",
        "        bg = Image.new('RGB', img.size, background_color)\n",
        "        bg.paste(img, mask=img.split()[-1])\n",
        "\n",
        "        if image_name.endswith('.webp'):\n",
        "            bg = bg.convert('RGB')\n",
        "            bg.save(f'{train_data_dir}/{image_name.replace(\".webp\", \".jpg\")}', \"JPEG\")\n",
        "            os.remove(f'{train_data_dir}/{image_name}')\n",
        "            print(f\" Converted image: {image_name} to {image_name.replace('.webp', '.jpg')}\")\n",
        "        else:\n",
        "            bg.save(f'{train_data_dir}/{image_name}', \"PNG\")\n",
        "            print(f\" Converted image: {image_name}\")\n",
        "    else:\n",
        "        if image_name.endswith('.webp'):\n",
        "            img.save(f'{train_data_dir}/{image_name.replace(\".webp\", \".jpg\")}', \"JPEG\")\n",
        "            os.remove(f'{train_data_dir}/{image_name}')\n",
        "            print(f\" Converted image: {image_name} to {image_name.replace('.webp', '.jpg')}\")\n",
        "        else:\n",
        "            img.save(f'{train_data_dir}/{image_name}', \"PNG\")\n",
        "\n",
        "num_batches = len(images) // batch_size + 1\n",
        "\n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    for i in tqdm(range(num_batches)):\n",
        "        start = i * batch_size\n",
        "        end = start + batch_size\n",
        "        batch = images[start:end]\n",
        "        executor.map(process_image, batch)\n",
        "\n",
        "print(\"All images have been converted\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "FzCGNU__caAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 4.2. Image Upscaler (Optional)\n",
        "from IPython.utils import capture\n",
        "%cd {root_dir}\n",
        "\n",
        "#@markdown Useful if your dataset has image with resolution lower than `512x512`, this will take some time and consume more gpu power.\n",
        "upscaler_model = \"RealESRGAN_x4plus_anime_6B\" #@param ['RealESRGAN_x4plus','RealESRNet_x4plus','RealESRGAN_x4plus_anime_6B','RealESRGAN_x2plus','realesr-animevideov3','realesr-general-x4v3']\n",
        "upcale_by = 2.5 #@param {type:\"slider\", min:1, max:5, step:0.1}\n",
        "face_enhance = False #@param {type:\"boolean\"}\n",
        "\n",
        "if not os.path.exists(f\"{root_dir}/Real-ESRGAN\"):\n",
        "  print(\"Installing dependencies, please wait...\")\n",
        "  with capture.capture_output() as cap:\n",
        "    !git clone https://github.com/xinntao/Real-ESRGAN.git\n",
        "    %cd {root_dir}/Real-ESRGAN\n",
        "    !pip -q install basicsr\n",
        "    !pip -q install facexlib\n",
        "    !pip -q install gfpgan\n",
        "    !pip -q install -r requirements.txt\n",
        "    !python setup.py develop\n",
        "    del cap\n",
        "\n",
        "%cd {root_dir}/Real-ESRGAN\n",
        "!python inference_realesrgan.py \\\n",
        "  -n \"{upscaler_model}\" \\\n",
        "  -i \"{train_data_dir}\" \\\n",
        "  -o \"{train_data_dir}\" \\\n",
        "  --suffix \"\" \\\n",
        "  --outscale {upcale_by} \\\n",
        "  {\"--face_enhance\" if face_enhance else \"\"}\n",
        "  "
      ],
      "metadata": {
        "cellView": "form",
        "id": "o8JLsTRkgDxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 4.3. Data Cleaning\n",
        "#@markdown This will delete unnecessary files and unsupported media like `.mp4`, `.webm`, and `.gif`\n",
        "%store -r\n",
        "\n",
        "import os\n",
        "\n",
        "%cd {root_dir}\n",
        "\n",
        "test = os.listdir(train_data_dir)\n",
        "\n",
        "#@markdown I recommend to `keep_metadata` especially if you're doing resume training and you have metadata and bucket latents file from previous training like `.npz`, `.txt`, `.caption`, and `json`.\n",
        "keep_metadata = True #@param {'type':'boolean'}\n",
        "\n",
        "if keep_metadata == True:\n",
        "  supported_types = [\".png\", \".jpg\", \".jpeg\", \".bmp\", \".webp\", \".caption\", \".npz\", \".txt\", \".json\"]\n",
        "else:\n",
        "  supported_types = [\".png\", \".jpg\", \".jpeg\", \".bmp\", \".webp\"]\n",
        "\n",
        "for item in test:\n",
        "    file_ext = os.path.splitext(item)[1]\n",
        "    if file_ext not in supported_types:\n",
        "        print(f\"Deleting file {item} from {train_data_dir}\")\n",
        "        os.remove(os.path.join(train_data_dir, item))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Ug648uiOvUZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4. Data Annotation (Optional)\n",
        "You can choose to train a model using caption. We're using [GIT: GenerativeImage2Text](https://huggingface.co/models?search=microsoft/git) for image captioning and [Waifu Diffusion 1.4 Tagger](https://huggingface.co/spaces/SmilingWolf/wd-v1-4-tags) for image tagging like danbooru.\n",
        "- Use GIT Captioning for: `Images in General`\n",
        "- Use Waifu Diffusion 1.4 Tagger V2 for: `Anime and manga-styled Images`"
      ],
      "metadata": {
        "id": "qdISafLeyklg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ### 4.4.1. GIT Captioning\n",
        "import glob\n",
        "%store -r\n",
        "\n",
        "%cd {finetune_dir}\n",
        "\n",
        "#@markdown [GIT: GenerativeImage2Text](https://huggingface.co/models?search=microsoft/git) is an image-to-text model published by [Microsoft](https://github.com/microsoft/GenerativeImage2Text). GIT is a Transformer decoder conditioned on both CLIP image tokens and text tokens. The model is trained using \"teacher forcing\" on a lot of (image, text) pairs. \n",
        "#@markdown Example: `astronaut riding a horse in space`. \n",
        "\n",
        "batch_size = 8 #@param {type:'number'}\n",
        "max_data_loader_n_workers = 2 #@param {type:'number'}\n",
        "model = \"microsoft/git-large-textcaps\" #@param [\"microsoft/git-base\", \"microsoft/git-large-coco\", \"microsoft/git-base-coco\", \"microsoft/git-base-vatex\", \"microsoft/git-base-textcaps\", \"microsoft/git-large-vatex\", \"microsoft/git-base-textvqa\", \"microsoft/git-large-textcaps\", \"microsoft/git-large-r\", \"microsoft/git-base-vqav2\", \"microsoft/git-large-r-textcaps\", \"microsoft/git-large\", \"microsoft/git-large-vqav2\", \"microsoft/git-large-textvqa\", \"microsoft/git-base-msrvtt-qa\", \"microsoft/git-large-msrvtt-qa\", \"microsoft/git-large-r-coco\"]\n",
        "max_length = 50 #@param {type:\"slider\", min:0, max:100, step:1.0}\n",
        "undesired_words = \"\" #@param {type:'string'}\n",
        "\n",
        "!python make_captions_by_git.py \\\n",
        "  \"{train_data_dir}\" \\\n",
        "  --model_id {model} \\\n",
        "  --batch_size {batch_size} \\\n",
        "  --max_length {max_length} \\\n",
        "  --remove_words {undesired_words} \\\n",
        "  --caption_extension .caption \\\n",
        "  --max_data_loader_n_workers {max_data_loader_n_workers}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "nvPyH-G_Qdha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ### 4.4.2. Waifu Diffusion 1.4 Tagger V2\n",
        "import glob\n",
        "%store -r\n",
        "\n",
        "%cd {finetune_dir}\n",
        "\n",
        "#@markdown [Waifu Diffusion 1.4 Tagger V2](https://huggingface.co/spaces/SmilingWolf/wd-v1-4-tags) is Danbooru-styled image classification developed by [SmilingWolf](https://github.com/SmilingWolf). It's intended to classify anime and manga-like artwork but can be useful for general use.\n",
        "#@markdown Example: `1girl, solo, looking_at_viewer, short_hair, bangs, simple_background`. \n",
        "batch_size = 8 #@param {type:'number'}\n",
        "max_data_loader_n_workers = 2 #@param {type:'number'}\n",
        "model = \"Swin_V2\" #@param [\"Swin_V2\", \"Convnext_V2\", \"ViT_V2\"]\n",
        "#@markdown It's recommended to set threshold higher (i.e. `0.85`) if you train on object or character, and lower the threshold (i.e `0.35`) to train on general, style or environment.\n",
        "threshold = 0.35 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "\n",
        "if model == \"Swin_V2\":\n",
        "  repo_id = \"SmilingWolf/wd-v1-4-swinv2-tagger-v2\"\n",
        "elif model == \"Convnext_V2\":\n",
        "  repo_id = \"SmilingWolf/wd-v1-4-convnext-tagger-v2\"\n",
        "else:\n",
        "  repo_id = \"SmilingWolf/wd-v1-4-vit-tagger-v2\"\n",
        "\n",
        "!python tag_images_by_wd14_tagger.py \\\n",
        "  \"{train_data_dir}\" \\\n",
        "  --batch_size {batch_size} \\\n",
        "  --repo_id {repo_id} \\\n",
        "  --thresh {threshold} \\\n",
        "  --caption_extension .txt \\\n",
        "  --max_data_loader_n_workers {max_data_loader_n_workers}\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "-BdXV7rAy2ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# V. Training Model\n",
        "\n"
      ],
      "metadata": {
        "id": "yHNbl3O_NSS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 5.1. Define Important folder\n",
        "from google.colab import drive\n",
        "%store -r\n",
        "\n",
        "v2 = False #@param {type:\"boolean\"}\n",
        "v_parameterization = False #@param {type:\"boolean\"}\n",
        "project_name = \"homie\" #@param {type:\"string\"}\n",
        "pretrained_model_name_or_path = \"/content/pre_trained_model/v1-5-pruned-emaonly.ckpt\" #@param {type:\"string\"}\n",
        "vae = \"/content/vae/stablediffusion.vae.pt\"  #@param {type:\"string\"}\n",
        "#@markdown You need to register parent folder and not where `train_data_dir` located\n",
        "train_folder_directory = \"/content/drive/MyDrive/homie\" #@param {'type':'string'}\n",
        "%store train_folder_directory\n",
        "reg_folder_directory = \"/content/drive/MyDrive/homie\" #@param {'type':'string'}\n",
        "%store reg_folder_directory\n",
        "output_dir = \"/content/drive/MyDrive\" #@param {'type':'string'}\n",
        "resume_path =\"\"\n",
        "inference_url = \"https://raw.githubusercontent.com/Stability-AI/stablediffusion/main/configs/stable-diffusion/\"\n",
        "\n",
        "#@markdown This will ignore `output_dir` defined above, and changed to `/content/drive/MyDrive/fine_tune/output` by default\n",
        "output_to_drive = False #@param {'type':'boolean'}\n",
        "\n",
        "if output_to_drive:\n",
        "  output_dir = \"/content/drive/MyDrive/dreambooth/output\"\n",
        "\n",
        "  if not os.path.exists(\"/content/drive\"):\n",
        "    drive.mount('/content/drive')  \n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "if v2 and not v_parameterization:\n",
        "  inference_url += \"v2-inference.yaml\"\n",
        "if v2 and v_parameterization:\n",
        "  inference_url += \"v2-inference-v.yaml\"\n",
        "\n",
        "try:\n",
        "  if v2:\n",
        "    !wget {inference_url} -O {output_dir}/{project_name}.yaml\n",
        "    print(\"File successfully downloaded\")\n",
        "except:\n",
        "  print(\"There was an error downloading the file. Please check the URL and try again.\")"
      ],
      "metadata": {
        "id": "H_Q23fUEJhnC",
        "cellView": "form",
        "outputId": "32fb0051-0785-430e-a2a5-654e181dcd92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stored 'train_folder_directory' (str)\n",
            "Stored 'reg_folder_directory' (str)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 5.2. Define Specific LoRA Training Parameters\n",
        "%store -r\n",
        "\n",
        "#@markdown ## LoRA - Low Rank Adaptation Dreambooth\n",
        "\n",
        "#@markdown Some people recommend setting the `network_dim` to a higher value.\n",
        "network_dim = 128 #@param {'type':'number'}\n",
        "#@markdown For weight scaling in LoRA, it is better to set `network_alpha` the same as `network_dim` unless you know what you're doing. A lower `network_alpha` requires a higher learning rate. For example, if `network_alpha = 1`, then `unet_lr = 1e-3`.\n",
        "network_alpha = 128 #@param {'type':'number'}\n",
        "network_module = \"networks.lora\"\n",
        "\n",
        "#@markdown `network_weights` can be specified to resume training.\n",
        "network_weights = \"\" #@param {'type':'string'}\n",
        "\n",
        "#@markdown By default, both Text Encoder and U-Net LoRA modules are enabled. Use `network_train_on` to specify which module to train.\n",
        "network_train_on = \"both\" #@param ['both','unet_only', 'text_encoder_only'] {'type':'string'}\n",
        "\n",
        "#@markdown It is recommended to set the `text_encoder_lr` to a lower learning rate, such as `5e-5`, or to set `text_encoder_lr = 1/2 * unet_lr`.\n",
        "learning_rate = 1e-4 #@param {'type':'number'}\n",
        "unet_lr = 0 #@param {'type':'number'}\n",
        "text_encoder_lr = 5e-5 #@param {'type':'number'}\n",
        "lr_scheduler = \"constant\" #@param [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"] {allow-input: false}\n",
        "\n",
        "#@markdown If `lr_scheduler = cosine_with_restarts`, update `lr_scheduler_num_cycles`.\n",
        "lr_scheduler_num_cycles = 1 #@param {'type':'number'}\n",
        "#@markdown If `lr_scheduler = polynomial`, update `lr_scheduler_power`.\n",
        "lr_scheduler_power = 1 #@param {'type':'number'}\n",
        "\n",
        "#@markdown Check the box to not save metadata in the output model.\n",
        "no_metadata = False #@param {type:\"boolean\"}\n",
        "training_comment = \"this comment will be stored in the metadata\" #@param {'type':'string'}\n",
        "\n",
        "print(\"Loading network module:\", network_module)\n",
        "print(f\"{network_module} dim set to:\", network_dim)\n",
        "print(f\"{network_module} alpha set to:\", network_alpha)\n",
        "\n",
        "if network_weights == \"\":\n",
        "  print(\"No LoRA weight loaded.\")\n",
        "else:\n",
        "  if os.path.exists(network_weights):\n",
        "    print(\"Loading LoRA weight:\", network_weights)\n",
        "  else:\n",
        "    print(f\"{network_weights} does not exist.\")\n",
        "    network_weights =\"\"\n",
        "\n",
        "if network_train_on == \"unet_only\":\n",
        "  print(\"Enabling LoRA for U-Net.\")\n",
        "  print(\"Disabling LoRA for Text Encoder.\")\n",
        "\n",
        "print(\"Global learning rate: \", learning_rate)\n",
        "\n",
        "if network_train_on == \"unet_only\":\n",
        "  print(\"Enable LoRA for U-Net\")\n",
        "  print(\"Disable LoRA for Text Encoder\")\n",
        "  print(\"UNet learning rate: \", unet_lr) if unet_lr != 0 else \"\"\n",
        "if network_train_on == \"text_encoder_only\":\n",
        "  print(\"Disabling LoRA for U-Net\")\n",
        "  print(\"Enabling LoRA for Text Encoder\")\n",
        "  print(\"Text encoder learning rate: \", text_encoder_lr) if text_encoder_lr != 0 else \"\"\n",
        "else:\n",
        "  print(\"Enabling LoRA for U-Net\")\n",
        "  print(\"Enabling LoRA for Text Encoder\")\n",
        "  print(\"UNet learning rate: \", unet_lr) if unet_lr != 0 else \"\"\n",
        "  print(\"Text encoder learning rate: \", text_encoder_lr) if text_encoder_lr != 0 else \"\"\n",
        "\n",
        "print(\"Learning rate Scheduler:\", lr_scheduler)\n",
        "\n",
        "if lr_scheduler == \"cosine_with_restarts\":\n",
        "  print(\"- Number of cycles: \", lr_scheduler_num_cycles)\n",
        "elif lr_scheduler == \"polynomial\":\n",
        "  print(\"- Power: \", lr_scheduler_power)\n",
        "\n",
        "# Printing the training comment if metadata is not disabled and a comment is present\n",
        "if not no_metadata:\n",
        "  if training_comment: \n",
        "    training_comment = training_comment.replace(\" \", \"_\")\n",
        "    print(\"Training comment:\", training_comment)\n",
        "else:\n",
        "  print(\"Metadata won't be saved\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "5P-QVvHMUrFB",
        "outputId": "ff43d0dc-e756-4089-b62f-b82ff1823537",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading network module: networks.lora\n",
            "networks.lora dim set to: 128\n",
            "networks.lora alpha set to: 128\n",
            "No LoRA weight loaded.\n",
            "Global learning rate:  0.0001\n",
            "Enabling LoRA for U-Net\n",
            "Enabling LoRA for Text Encoder\n",
            "Text encoder learning rate:  5e-05\n",
            "Learning rate Scheduler: constant\n",
            "Training comment: this_comment_will_be_stored_in_the_metadata\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from prettytable import PrettyTable\n",
        "import textwrap\n",
        "import yaml\n",
        "\n",
        "%store -r\n",
        "\n",
        "#@title ## 5.3. Start LoRA Dreambooth\n",
        "#@markdown ### Define Parameter\n",
        "\n",
        "train_batch_size = 6 #@param {type:\"number\"}\n",
        "num_epochs = 8 #@param {type:\"number\"}\n",
        "caption_extension = '.txt' #@param {'type':'string'}\n",
        "mixed_precision = \"fp16\" #@param [\"no\",\"fp16\",\"bf16\"] {allow-input: false}\n",
        "save_precision = \"fp16\" #@param [\"float\", \"fp16\", \"bf16\"] {allow-input: false}\n",
        "save_n_epochs_type = \"save_n_epoch_ratio\" #@param [\"save_every_n_epochs\", \"save_n_epoch_ratio\"] {allow-input: false}\n",
        "save_n_epochs_type_value = 1 #@param {type:\"number\"}\n",
        "save_model_as = \"safetensors\" #@param [\"ckpt\", \"pt\", \"safetensors\"] {allow-input: false}\n",
        "resolution = 512 #@param {type:\"slider\", min:512, max:1024, step:128}\n",
        "enable_bucket = True #@param {type:\"boolean\"}\n",
        "min_bucket_reso = 320 if resolution > 640 else 256\n",
        "max_bucket_reso = 1280 if resolution > 640 else 1024\n",
        "cache_latents = True #@param {type:\"boolean\"}\n",
        "max_token_length = 225 #@param {type:\"number\"}\n",
        "clip_skip = 2 #@param {type:\"number\"}\n",
        "use_8bit_adam = True #@param {type:\"boolean\"}\n",
        "gradient_checkpointing = False #@param {type:\"boolean\"}\n",
        "gradient_accumulation_steps = 1 #@param {type:\"number\"}\n",
        "seed = 0 #@param {type:\"number\"}\n",
        "logging_dir = \"/content/dreambooth/logs\"\n",
        "log_prefix = project_name\n",
        "additional_argument = \"--shuffle_caption --xformers\" #@param {type:\"string\"}\n",
        "print_hyperparameter = True #@param {type:\"boolean\"}\n",
        "prior_loss_weight = 1.0\n",
        "%cd {repo_dir}\n",
        "\n",
        "train_command=f\"\"\"\n",
        "accelerate launch --config_file={accelerate_config} --num_cpu_threads_per_process=8 train_network.py \\\n",
        "  {\"--v2\" if v2 else \"\"} \\\n",
        "  {\"--v_parameterization\" if v2 and v_parameterization else \"\"} \\\n",
        "  --network_dim={network_dim} \\\n",
        "  --network_alpha={network_alpha} \\\n",
        "  --network_module={network_module} \\\n",
        "  {\"--network_weights=\" + network_weights if network_weights else \"\"} \\\n",
        "  {\"--network_train_unet_only\" if network_train_on == \"unet_only\" else \"\"} \\\n",
        "  {\"--network_train_text_encoder_only\" if network_train_on == \"text_encoder_only\" else \"\"} \\\n",
        "  --learning_rate={learning_rate} \\\n",
        "  {\"--unet_lr=\" + format(unet_lr) if unet_lr !=0 else \"\"} \\\n",
        "  {\"--text_encoder_lr=\" + format(text_encoder_lr) if text_encoder_lr !=0 else \"\"} \\\n",
        "  {\"--no_metadata\" if no_metadata else \"\"} \\\n",
        "  {\"--training_comment=\" + training_comment if training_comment and not no_metadata else \"\"} \\\n",
        "  --lr_scheduler={lr_scheduler} \\\n",
        "  {\"--lr_scheduler_num_cycles=\" + format(lr_scheduler_num_cycles) if lr_scheduler == \"cosine_with_restarts\" else \"\"} \\\n",
        "  {\"--lr_scheduler_power=\" + format(lr_scheduler_power) if lr_scheduler == \"polynomial\" else \"\"} \\\n",
        "  --pretrained_model_name_or_path={pretrained_model_name_or_path} \\\n",
        "  {\"--vae=\" + vae if vae else \"\"} \\\n",
        "  {\"--caption_extension=\" + caption_extension if caption_extension else \"\"} \\\n",
        "  --train_data_dir={train_folder_directory} \\\n",
        "  --reg_data_dir={reg_folder_directory} \\\n",
        "  --output_dir={output_dir} \\\n",
        "  --prior_loss_weight={prior_loss_weight} \\\n",
        "  {\"--resume=\" + resume_path if resume_path else \"\"} \\\n",
        "  {\"--output_name=\" + project_name if project_name else \"\"} \\\n",
        "  --mixed_precision={mixed_precision} \\\n",
        "  --save_precision={save_precision} \\\n",
        "  {\"--save_every_n_epochs=\" + format(save_n_epochs_type_value) if save_n_epochs_type==\"save_every_n_epochs\" else \"\"} \\\n",
        "  {\"--save_n_epoch_ratio=\" + format(save_n_epochs_type_value) if save_n_epochs_type==\"save_n_epoch_ratio\" else \"\"} \\\n",
        "  --save_model_as={save_model_as} \\\n",
        "  --resolution={resolution} \\\n",
        "  {\"--enable_bucket\" if enable_bucket else \"\"} \\\n",
        "  {\"--min_bucket_reso=\" + format(min_bucket_reso) if enable_bucket else \"\"} \\\n",
        "  {\"--max_bucket_reso=\" + format(max_bucket_reso) if enable_bucket else \"\"} \\\n",
        "  {\"--cache_latents\" if cache_latents else \"\"} \\\n",
        "  --train_batch_size={train_batch_size} \\\n",
        "  --max_token_length={max_token_length} \\\n",
        "  {\"--use_8bit_adam\" if use_8bit_adam else \"\"} \\\n",
        "  --max_train_epochs={num_epochs} \\\n",
        "  {\"--seed=\" + format(seed) if seed > 0 else \"\"} \\\n",
        "  {\"--gradient_checkpointing\" if gradient_checkpointing else \"\"} \\\n",
        "  {\"--gradient_accumulation_steps=\" + format(gradient_accumulation_steps) } \\\n",
        "  {\"--clip_skip=\" + format(clip_skip) if v2 == False else \"\"} \\\n",
        "  --logging_dir={logging_dir} \\\n",
        "  --log_prefix={log_prefix} \\\n",
        "  {additional_argument}\n",
        "  \"\"\"\n",
        "\n",
        "debug_params = [\"v2\", \\\n",
        "                \"v_parameterization\", \\\n",
        "                \"network_dim\", \\\n",
        "                \"network_alpha\", \\\n",
        "                \"network_module\", \\\n",
        "                \"network_weights\", \\\n",
        "                \"network_train_on\", \\\n",
        "                \"learning_rate\", \\\n",
        "                \"unet_lr\", \\\n",
        "                \"text_encoder_lr\", \\\n",
        "                \"no_metadata\", \\\n",
        "                \"training_comment\", \\\n",
        "                \"lr_scheduler\", \\\n",
        "                \"lr_scheduler_num_cycles\", \\\n",
        "                \"lr_scheduler_power\", \\\n",
        "                \"pretrained_model_name_or_path\", \\\n",
        "                \"vae\", \\\n",
        "                \"caption_extension\", \\\n",
        "                \"train_folder_directory\", \\\n",
        "                \"reg_folder_directory\", \\\n",
        "                \"output_dir\", \\\n",
        "                \"prior_loss_weight\", \\\n",
        "                \"resume_path\", \\\n",
        "                \"project_name\", \\\n",
        "                \"mixed_precision\", \\\n",
        "                \"save_precision\", \\\n",
        "                \"save_n_epochs_type\", \\\n",
        "                \"save_n_epochs_type_value\", \\\n",
        "                \"save_model_as\", \\\n",
        "                \"resolution\", \\\n",
        "                \"enable_bucket\", \\\n",
        "                \"min_bucket_reso\", \\\n",
        "                \"max_bucket_reso\", \\\n",
        "                \"cache_latents\", \\\n",
        "                \"train_batch_size\", \\\n",
        "                \"max_token_length\", \\\n",
        "                \"use_8bit_adam\", \\\n",
        "                \"num_epochs\", \\\n",
        "                \"seed\", \\\n",
        "                \"gradient_checkpointing\", \\\n",
        "                \"gradient_accumulation_steps\", \\\n",
        "                \"clip_skip\", \\\n",
        "                \"logging_dir\", \\\n",
        "                \"log_prefix\", \\\n",
        "                \"additional_argument\"]\n",
        "\n",
        "if print_hyperparameter:\n",
        "    table = PrettyTable()\n",
        "    table.field_names = [\"Hyperparameter\", \"Value\"]\n",
        "    for params in debug_params:\n",
        "        if params != \"\":\n",
        "            if globals()[params] == \"\":\n",
        "                value = \"False\"\n",
        "            else:\n",
        "                value = globals()[params]\n",
        "            table.add_row([params, value])\n",
        "    table.align = \"l\"\n",
        "    print(table)\n",
        "\n",
        "    arg_list = train_command.split()\n",
        "    mod_train_command = {'command': arg_list}\n",
        "    \n",
        "    train_folder = os.path.dirname(output_dir)\n",
        "    \n",
        "\n",
        "f = open(\"./train.sh\", \"w\")\n",
        "f.write(train_command)\n",
        "f.close()\n",
        "!chmod +x ./train.sh\n",
        "!./train.sh"
      ],
      "metadata": {
        "cellView": "form",
        "id": "GIvfcS1pOrGY",
        "outputId": "0d56aab4-3290-45ef-9c24-f45e86ece1c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/kohya-trainer\n",
            "+-------------------------------+-----------------------------------------------------+\n",
            "| Hyperparameter                | Value                                               |\n",
            "+-------------------------------+-----------------------------------------------------+\n",
            "| v2                            | False                                               |\n",
            "| v_parameterization            | False                                               |\n",
            "| network_dim                   | 128                                                 |\n",
            "| network_alpha                 | 128                                                 |\n",
            "| network_module                | networks.lora                                       |\n",
            "| network_weights               | False                                               |\n",
            "| network_train_on              | both                                                |\n",
            "| learning_rate                 | 0.0001                                              |\n",
            "| unet_lr                       | 0                                                   |\n",
            "| text_encoder_lr               | 5e-05                                               |\n",
            "| no_metadata                   | False                                               |\n",
            "| training_comment              | this_comment_will_be_stored_in_the_metadata         |\n",
            "| lr_scheduler                  | constant                                            |\n",
            "| lr_scheduler_num_cycles       | 1                                                   |\n",
            "| lr_scheduler_power            | 1                                                   |\n",
            "| pretrained_model_name_or_path | /content/pre_trained_model/v1-5-pruned-emaonly.ckpt |\n",
            "| vae                           | /content/vae/stablediffusion.vae.pt                 |\n",
            "| caption_extension             | .txt                                                |\n",
            "| train_folder_directory        | /content/drive/MyDrive/homie                        |\n",
            "| reg_folder_directory          | /content/drive/MyDrive/homie                        |\n",
            "| output_dir                    | /content/drive/MyDrive                              |\n",
            "| prior_loss_weight             | 1.0                                                 |\n",
            "| resume_path                   | False                                               |\n",
            "| project_name                  | homie                                               |\n",
            "| mixed_precision               | fp16                                                |\n",
            "| save_precision                | fp16                                                |\n",
            "| save_n_epochs_type            | save_n_epoch_ratio                                  |\n",
            "| save_n_epochs_type_value      | 4                                                   |\n",
            "| save_model_as                 | safetensors                                         |\n",
            "| resolution                    | 512                                                 |\n",
            "| enable_bucket                 | True                                                |\n",
            "| min_bucket_reso               | 256                                                 |\n",
            "| max_bucket_reso               | 1024                                                |\n",
            "| cache_latents                 | True                                                |\n",
            "| train_batch_size              | 6                                                   |\n",
            "| max_token_length              | 225                                                 |\n",
            "| use_8bit_adam                 | True                                                |\n",
            "| num_epochs                    | 8                                                   |\n",
            "| seed                          | 0                                                   |\n",
            "| gradient_checkpointing        | False                                               |\n",
            "| gradient_accumulation_steps   | 1                                                   |\n",
            "| clip_skip                     | 2                                                   |\n",
            "| logging_dir                   | /content/dreambooth/logs                            |\n",
            "| log_prefix                    | homie                                               |\n",
            "| additional_argument           | --shuffle_caption --xformers                        |\n",
            "+-------------------------------+-----------------------------------------------------+\n",
            "2023-04-08 16:24:14.229625: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-04-08 16:24:15.246601: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-04-08 16:24:15.246768: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-04-08 16:24:15.246794: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2023-04-08 16:24:18.873320: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-04-08 16:24:19.646111: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-04-08 16:24:19.646214: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-04-08 16:24:19.646233: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "prepare tokenizer\n",
            "Downloading (…)olve/main/vocab.json: 100% 961k/961k [00:00<00:00, 11.3MB/s]\n",
            "Downloading (…)olve/main/merges.txt: 100% 525k/525k [00:00<00:00, 2.61MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 389/389 [00:00<00:00, 145kB/s]\n",
            "Downloading (…)okenizer_config.json: 100% 905/905 [00:00<00:00, 277kB/s]\n",
            "update token length: 225\n",
            "Use DreamBooth method.\n",
            "prepare images.\n",
            "found directory /content/drive/MyDrive/homie/10_homie contains 8 image files\n",
            "ignore duplicated subset with image_dir='/content/drive/MyDrive/homie/10_homie': use the first one / 既にサブセットが登録されているため、重複した後発のサブセットを無視します\n",
            "80 train images with repeating.\n",
            "0 reg images.\n",
            "no regularization images / 正則化画像が見つかりませんでした\n",
            "[Dataset 0]\n",
            "  batch_size: 6\n",
            "  resolution: (512, 512)\n",
            "  enable_bucket: True\n",
            "  min_bucket_reso: 256\n",
            "  max_bucket_reso: 1024\n",
            "  bucket_reso_steps: 64\n",
            "  bucket_no_upscale: False\n",
            "\n",
            "  [Subset 0 of Dataset 0]\n",
            "    image_dir: \"/content/drive/MyDrive/homie/10_homie\"\n",
            "    image_count: 8\n",
            "    num_repeats: 10\n",
            "    shuffle_caption: True\n",
            "    keep_tokens: 0\n",
            "    caption_dropout_rate: 0.0\n",
            "    caption_dropout_every_n_epoches: 0\n",
            "    caption_tag_dropout_rate: 0.0\n",
            "    color_aug: False\n",
            "    flip_aug: False\n",
            "    face_crop_aug_range: None\n",
            "    random_crop: False\n",
            "    is_reg: False\n",
            "    class_tokens: homie\n",
            "    caption_extension: .txt\n",
            "\n",
            "\n",
            "[Dataset 0]\n",
            "loading image sizes.\n",
            "100% 8/8 [00:05<00:00,  1.60it/s]\n",
            "make buckets\n",
            "number of images (including repeats) / 各bucketの画像枚数（繰り返し回数を含む）\n",
            "bucket 0: resolution (512, 512), count: 80\n",
            "mean ar error (without repeats): 0.0\n",
            "prepare accelerator\n",
            "Using accelerator 0.15.0 or above.\n",
            "load StableDiffusion checkpoint\n",
            "loading u-net: <All keys matched successfully>\n",
            "loading vae: <All keys matched successfully>\n",
            "Downloading (…)lve/main/config.json: 100% 4.52k/4.52k [00:00<00:00, 628kB/s]\n",
            "Downloading (…)\"pytorch_model.bin\";: 100% 1.71G/1.71G [00:10<00:00, 160MB/s]\n",
            "loading text encoder: <All keys matched successfully>\n",
            "load VAE: /content/vae/stablediffusion.vae.pt\n",
            "additional VAE loaded\n",
            "Replace CrossAttention.forward to use xformers\n",
            "[Dataset 0]\n",
            "caching latents.\n",
            "100% 8/8 [00:08<00:00,  1.05s/it]\n",
            "import network module: networks.lora\n",
            "create LoRA network. base dim (rank): 128, alpha: 128.0\n",
            "create LoRA for Text Encoder: 72 modules.\n",
            "create LoRA for U-Net: 192 modules.\n",
            "enable LoRA for text encoder\n",
            "enable LoRA for U-Net\n",
            "prepare optimizer, data loader etc.\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "For effortless bug reporting copy-paste your error into this form: https://docs.google.com/forms/d/e/1FAIpQLScPB8emS3Thkp66nvqwmjTEgxp8Y9ufuWTzFyr9kJ5AoI47dQ/viewform?usp=sf_link\n",
            "================================================================================\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "use 8-bit AdamW optimizer | {}\n",
            "override steps. steps for 8 epochs is / 指定エポックまでのステップ数: 112\n",
            "running training / 学習開始\n",
            "  num train images * repeats / 学習画像の数×繰り返し回数: 80\n",
            "  num reg images / 正則化画像の数: 0\n",
            "  num batches per epoch / 1epochのバッチ数: 14\n",
            "  num epochs / epoch数: 8\n",
            "  batch size per device / バッチサイズ: 6\n",
            "  gradient accumulation steps / 勾配を合計するステップ数 = 1\n",
            "  total optimization steps / 学習ステップ数: 112\n",
            "steps:   0% 0/112 [00:00<?, ?it/s]epoch 1/8\n",
            "steps:  12% 14/112 [00:20<02:26,  1.50s/it, loss=0.133]epoch 2/8\n",
            "steps:  25% 28/112 [00:40<02:02,  1.46s/it, loss=0.101]saving checkpoint: /content/drive/MyDrive/homie-000002.safetensors\n",
            "epoch 3/8\n",
            "steps:  38% 42/112 [01:02<01:44,  1.49s/it, loss=0.108]epoch 4/8\n",
            "steps:  50% 56/112 [01:23<01:23,  1.49s/it, loss=0.109]saving checkpoint: /content/drive/MyDrive/homie-000004.safetensors\n",
            "epoch 5/8\n",
            "steps:  62% 70/112 [01:46<01:03,  1.52s/it, loss=0.0979]epoch 6/8\n",
            "steps:  75% 84/112 [02:07<00:42,  1.52s/it, loss=0.0902]saving checkpoint: /content/drive/MyDrive/homie-000006.safetensors\n",
            "epoch 7/8\n",
            "steps:  88% 98/112 [02:30<00:21,  1.53s/it, loss=0.11] epoch 8/8\n",
            "steps: 100% 112/112 [02:51<00:00,  1.53s/it, loss=0.105]save trained model to /content/drive/MyDrive/homie.safetensors\n",
            "model saved.\n",
            "steps: 100% 112/112 [02:52<00:00,  1.54s/it, loss=0.105]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VI. Testing"
      ],
      "metadata": {
        "id": "reMcN0bM_o53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 6.1. Validating LoRA Weights\n",
        "\n",
        "#@markdown Now you can check if your LoRA trained properly. \n",
        "\n",
        "#@markdown  If you used `clip_skip = 2` during training, the values of `lora_te_text_model_encoder_layers_11_*` will be `0.0`, this is normal. These layers are not trained at this value of `Clip Skip`.\n",
        "network_weights = \"/content/dreambooth/output/neurosama.safetensors\" #@param {'type':'string'}\n",
        "no_verbose = True #@param {type:\"boolean\"}\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import json\n",
        "from safetensors.torch import load_file\n",
        "from safetensors.torch import safe_open\n",
        "import library.model_util as model_util\n",
        "\n",
        "print(\"Loading LoRA weight:\", network_weights)\n",
        "\n",
        "def main(file, verbose:bool):\n",
        "  if not verbose:\n",
        "    sd = load_file(file) if os.path.splitext(file)[1] == '.safetensors' else torch.load(file, map_location='cuda')\n",
        "    values = []\n",
        "\n",
        "    keys = list(sd.keys())\n",
        "    for key in keys:\n",
        "      if 'lora_up' in key or 'lora_down' in key:\n",
        "        values.append((key, sd[key]))\n",
        "    print(f\"number of LoRA modules: {len(values)}\")\n",
        "\n",
        "    for key, value in values:\n",
        "      value = value.to(torch.float32)\n",
        "      print(f\"{key},{torch.mean(torch.abs(value))},{torch.min(torch.abs(value))}\")\n",
        "  \n",
        "  if model_util.is_safetensors(file):\n",
        "      with safe_open(file, framework=\"pt\") as f:\n",
        "          metadata = f.metadata()\n",
        "      if metadata is not None:\n",
        "        if not verbose:\n",
        "          metadata[\"ss_bucket_info\"] = json.loads(metadata[\"ss_bucket_info\"].replace(\"\\\\\", \"\"))\n",
        "          metadata[\"ss_tag_frequency\"] = json.loads(metadata[\"ss_tag_frequency\"].replace(\"\\\\\", \"\"))\n",
        "        metadata[\"ss_dataset_dirs\"] = json.loads(metadata[\"ss_dataset_dirs\"].replace(\"\\\\\", \"\"))\n",
        "        metadata[\"ss_reg_dataset_dirs\"] = json.loads(metadata[\"ss_reg_dataset_dirs\"].replace(\"\\\\\", \"\"))\n",
        "        print(f\"\\nLoad metadata for: {file}\")\n",
        "        print(json.dumps(metadata, indent=4))\n",
        "  else:\n",
        "      print(\"No metadata saved, your model is not in safetensors format\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main(network_weights, no_verbose)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Rt7CKCog_4tm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 6.2. Inference\n",
        "%store -r\n",
        "\n",
        "#@markdown LoRA Config\n",
        "network_weights = \"/content/dreambooth/output/neurosama.safetensors\" #@param {'type':'string'}\n",
        "network_module = \"networks.lora\"\n",
        "network_mul = 0.6 #@param {'type':'number'}\n",
        "\n",
        "#@markdown Other Config\n",
        "v2 = False #@param {type:\"boolean\"}\n",
        "v_parameterization = False #@param {type:\"boolean\"}\n",
        "instance_prompt = \"\" #@param {type: \"string\"}\n",
        "prompt = \"masterpiece, best quality, 1girl, aqua eyes, baseball cap, blonde hair, closed mouth, earrings, green background, hat, hoop earrings, jewelry, looking at viewer, shirt, short hair, simple background, solo, upper body, yellow shirt\" #@param {type: \"string\"}\n",
        "negative = \"lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry\" #@param {type: \"string\"}\n",
        "model = \"/content/pre_trained_model/Anything-v3-1.safetensors\" #@param {type: \"string\"}\n",
        "vae = \"\" #@param {type: \"string\"}\n",
        "outdir = \"/content/tmp\" #@param {type: \"string\"}\n",
        "scale = 5 #@param {type: \"slider\", min: 1, max: 40}\n",
        "sampler = \"ddim\" #@param [\"ddim\", \"pndm\", \"lms\", \"euler\", \"euler_a\", \"heun\", \"dpm_2\", \"dpm_2_a\", \"dpmsolver\",\"dpmsolver++\", \"dpmsingle\", \"k_lms\", \"k_euler\", \"k_euler_a\", \"k_dpm_2\", \"k_dpm_2_a\"]\n",
        "steps = 28 #@param {type: \"slider\", min: 1, max: 100}\n",
        "precision = \"fp16\" #@param [\"fp16\", \"bf16\"] {allow-input: false}\n",
        "width = 512 #@param {type: \"integer\"}\n",
        "height = 768 #@param {type: \"integer\"}\n",
        "images_per_prompt = 4 #@param {type: \"integer\"}\n",
        "batch_size = 4 #@param {type: \"integer\"}\n",
        "clip_skip = 2 #@param {type: \"slider\", min: 1, max: 40}\n",
        "seed = -1 #@param {type: \"integer\"}\n",
        "\n",
        "final_prompt = f\"{instance_prompt}, {prompt} --n {negative}\" if instance_prompt else f\"{prompt} --n {negative}\"\n",
        "\n",
        "%cd {repo_dir}\n",
        "\n",
        "!python gen_img_diffusers.py \\\n",
        "  {\"--v2\" if v2 else \"\"} \\\n",
        "  {\"--v_parameterization\" if v2 and v_parameterization else \"\"} \\\n",
        "  --network_module={network_module} \\\n",
        "  --network_weight={network_weights} \\\n",
        "  --network_mul={network_mul} \\\n",
        "  --ckpt={model} \\\n",
        "  --outdir={outdir} \\\n",
        "  --xformers \\\n",
        "  {\"--vae=\" + vae if vae else \"\"} \\\n",
        "  --{precision} \\\n",
        "  --W={width} \\\n",
        "  --H={height} \\\n",
        "  {\"--seed=\" + format(seed) if seed > 0 else \"\"} \\\n",
        "  --scale={scale} \\\n",
        "  --sampler={sampler} \\\n",
        "  --steps={steps} \\\n",
        "  --max_embeddings_multiples=3 \\\n",
        "  --batch_size={batch_size} \\\n",
        "  --images_per_prompt={images_per_prompt} \\\n",
        "  {\"--clip_skip=\" + format(clip_skip) if v2 == False else \"\"} \\\n",
        "  --prompt=\"{final_prompt}\"\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "FKBrTDPrcNjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 6.3. Visualize loss graph (Optional)\n",
        "training_logs_path = \"/content/dreambooth/logs\" #@param {type : \"string\"}\n",
        "\n",
        "%cd /content/kohya-trainer\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {training_logs_path}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "TLSQslfFcQde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VII. Extras"
      ],
      "metadata": {
        "id": "N6ckzE2GWudi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 7.1. Compressing model or dataset\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "zip_module = \"zipfile\" #@param [\"zipfile\", \"shutil\", \"pyminizip\", \"zip\"]\n",
        "directory_to_zip = '/content/dreambooth/output' #@param {type: \"string\"}\n",
        "output_filename = '/content/neurosama.zip' #@param {type: \"string\"}\n",
        "password = \"\" #@param {type: \"string\"}\n",
        "\n",
        "if zip_module == \"zipfile\":\n",
        "    with zipfile.ZipFile(output_filename, 'w') as zip:\n",
        "        for directory_to_zip, dirs, files in os.walk(directory_to_zip):\n",
        "            for file in files:\n",
        "                zip.write(os.path.join(directory_to_zip, file))\n",
        "elif zip_module == \"shutil\":\n",
        "    shutil.make_archive(output_filename, 'zip', directory_to_zip)\n",
        "elif zip_module == \"pyminizip\":\n",
        "    !pip install pyminizip\n",
        "    import pyminizip\n",
        "    for root, dirs, files in os.walk(directory_to_zip):\n",
        "        for file in files:\n",
        "            pyminizip.compress(os.path.join(root, file), \"\", os.path.join(\"*\",output_filename), password, 5)\n",
        "elif zip_module == \"zip\":\n",
        "    !zip -rv -q -j {output_filename} {directory_to_zip}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "rLdEpPKTbI1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VIII. Deployment"
      ],
      "metadata": {
        "id": "nyIl9BhNXKUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 8.1. Define your Huggingface Repo\n",
        "\n",
        "from huggingface_hub import HfApi\n",
        "from huggingface_hub.utils import validate_repo_id, HfHubHTTPError\n",
        "%store -r\n",
        "\n",
        "api = HfApi()\n",
        "user = api.whoami(write_token)\n",
        "\n",
        "#@markdown #### Fill this if you want to upload to your organization, or just leave it empty.\n",
        "\n",
        "orgs_name = \"\" #@param{type:\"string\"}\n",
        "\n",
        "#@markdown #### If your model/dataset repo didn't exist, it will automatically create your repo.\n",
        "model_name = \"your-model-name\" #@param{type:\"string\"}\n",
        "dataset_name = \"your-dataset-name\" #@param{type:\"string\"}\n",
        "make_this_model_private = True #@param{type:\"boolean\"}\n",
        "\n",
        "if orgs_name == \"\":\n",
        "  model_repo = user['name']+\"/\"+model_name.strip()\n",
        "  datasets_repo = user['name']+\"/\"+dataset_name.strip()\n",
        "else:\n",
        "  model_repo = orgs_name+\"/\"+model_name.strip()\n",
        "  datasets_repo = orgs_name+\"/\"+dataset_name.strip()\n",
        "\n",
        "if model_name != \"\":\n",
        "  try:\n",
        "      validate_repo_id(model_repo)\n",
        "      api.create_repo(repo_id=model_repo, \n",
        "                      private=make_this_model_private)\n",
        "      print(\"Model Repo didn't exists, creating repo\")\n",
        "      print(\"Model Repo: \",model_repo,\"created!\\n\")\n",
        "\n",
        "  except HfHubHTTPError as e:\n",
        "      print(f\"Model Repo: {model_repo} exists, skipping create repo\\n\")\n",
        "\n",
        "if dataset_name != \"\":\n",
        "  try:\n",
        "      validate_repo_id(datasets_repo)\n",
        "      api.create_repo(repo_id=datasets_repo,\n",
        "                      repo_type=\"dataset\",\n",
        "                      private=make_this_model_private)\n",
        "      print(\"Dataset Repo didn't exists, creating repo\")\n",
        "      print(\"Dataset Repo\",datasets_repo,\"created!\\n\")\n",
        "\n",
        "  except HfHubHTTPError as e:\n",
        "      print(f\"Dataset repo: {datasets_repo} exists, skipping create repo\\n\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "QTXsM170GUpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.2. Upload with `hf_hub`"
      ],
      "metadata": {
        "id": "yUNkWbMHcbiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ### 8.2.1. Upload LoRA\n",
        "from huggingface_hub import HfApi\n",
        "from pathlib import Path\n",
        "%store -r\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "#@markdown #### This will be uploaded to model repo\n",
        "model_path = \"/content/dreambooth/output\" #@param {type :\"string\"}\n",
        "path_in_repo = \"neurosama_v2\" #@param {type :\"string\"}\n",
        "\n",
        "#@markdown #### Other Information\n",
        "commit_message = \"\" #@param {type :\"string\"}\n",
        "\n",
        "if not commit_message:\n",
        "  commit_message = \"feat: upload \"+project_name+\" lora model\"\n",
        "def upload_model(model_paths, is_folder :bool):\n",
        "  path_obj = Path(model_paths)\n",
        "  trained_model = path_obj.parts[-1]\n",
        "  \n",
        "  if path_in_repo:\n",
        "    trained_model = path_in_repo\n",
        "    \n",
        "  if is_folder == True:\n",
        "    print(f\"Uploading {trained_model} to https://huggingface.co/\"+model_repo)\n",
        "    print(f\"Please wait...\")\n",
        "    \n",
        "    api.upload_folder(\n",
        "        folder_path=model_paths,\n",
        "        path_in_repo=trained_model,\n",
        "        repo_id=model_repo,\n",
        "        commit_message=commit_message,\n",
        "        ignore_patterns=\".ipynb_checkpoints\"\n",
        "        )\n",
        "    print(f\"Upload success, located at https://huggingface.co/\"+model_repo+\"/tree/main\\n\")\n",
        "  else: \n",
        "    print(f\"Uploading {trained_model} to https://huggingface.co/\"+model_repo)\n",
        "    print(f\"Please wait...\")\n",
        "            \n",
        "    api.upload_file(\n",
        "        path_or_fileobj=model_paths,\n",
        "        path_in_repo=trained_model,\n",
        "        repo_id=model_repo,\n",
        "        commit_message=commit_message,\n",
        "        )\n",
        "        \n",
        "    print(f\"Upload success, located at https://huggingface.co/\"+model_repo+\"/blob/main/\"+trained_model+\"\\n\")\n",
        "      \n",
        "def upload():\n",
        "    if model_path.endswith((\".ckpt\", \".safetensors\", \".pt\")):\n",
        "      upload_model(model_path, False)\n",
        "    else:\n",
        "      upload_model(model_path, True)\n",
        "\n",
        "upload()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "CIeoJA-eO-8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ### 8.2.2. Upload Dataset\n",
        "from huggingface_hub import HfApi\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import zipfile\n",
        "import os\n",
        "%store -r\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "#@markdown #### This will be compressed to zip and  uploaded to datasets repo, leave it empty if not necessary\n",
        "train_data_path = \"/content/dreambooth/train_data\" #@param {type :\"string\"}\n",
        "#@markdown ##### `Nerd stuff, only if you want to save training logs`\n",
        "logs_path = \"/content/dreambooth/logs\" #@param {type :\"string\"}\n",
        "#@markdown #### Delete zip after upload\n",
        "delete_zip = True #@param {type :\"boolean\"}\n",
        "\n",
        "if project_name !=\"\":\n",
        "  tmp_dataset = \"/content/dreambooth/\"+project_name+\"_dataset\"\n",
        "else:\n",
        "  tmp_dataset = \"/content/dreambooth/tmp_dataset\"\n",
        "\n",
        "dataset_zip = tmp_dataset + \".zip\"\n",
        "\n",
        "#@markdown #### Other Information\n",
        "commit_message = \"\" #@param {type :\"string\"}\n",
        "\n",
        "if not commit_message:\n",
        "  commit_message = \"feat: upload \"+project_name+\" dataset and logs\"\n",
        "\n",
        "os.makedirs(tmp_dataset)\n",
        "\n",
        "def upload_dataset(dataset_paths, is_zip : bool):\n",
        "  path_obj = Path(dataset_paths)\n",
        "  dataset_name = path_obj.parts[-1]\n",
        "\n",
        "  if is_zip:\n",
        "    print(f\"Uploading {dataset_name} to https://huggingface.co/datasets/\"+datasets_repo)\n",
        "    print(f\"Please wait...\")\n",
        "\n",
        "    api.upload_file(\n",
        "        path_or_fileobj=dataset_paths,\n",
        "        path_in_repo=dataset_name,\n",
        "        repo_id=datasets_repo,\n",
        "        repo_type=\"dataset\",\n",
        "        commit_message=commit_message,\n",
        "    )\n",
        "    print(f\"Upload success, located at https://huggingface.co/datasets/\"+datasets_repo+\"/blob/main/\"+dataset_name+\"\\n\")\n",
        "  else:\n",
        "    print(f\"Uploading {dataset_name} to https://huggingface.co/datasets/\"+datasets_repo)\n",
        "    print(f\"Please wait...\")\n",
        "\n",
        "    api.upload_folder(\n",
        "        folder_path=dataset_paths,\n",
        "        path_in_repo=dataset_name,\n",
        "        repo_id=datasets_repo,\n",
        "        repo_type=\"dataset\",\n",
        "        commit_message=commit_message,\n",
        "        ignore_patterns=\".ipynb_checkpoints\",\n",
        "    )\n",
        "    print(f\"Upload success, located at https://huggingface.co/datasets/\"+datasets_repo+\"/tree/main/\"+dataset_name+\"\\n\")\n",
        "  \n",
        "def zip_file(tmp):\n",
        "    zipfiles = tmp + \".zip\" \n",
        "    with zipfile.ZipFile(zipfiles, 'w') as zip:\n",
        "      for tmp, dirs, files in os.walk(tmp):\n",
        "          for file in files:\n",
        "              zip.write(os.path.join(tmp, file))\n",
        "\n",
        "def move(src_path, dst_path, is_metadata: bool):\n",
        "  files_to_move = [\"meta_cap.json\", \\\n",
        "                   \"meta_cap_dd.json\", \\\n",
        "                   \"meta_lat.json\", \\\n",
        "                   \"meta_clean.json\", \\\n",
        "                   \"meta_final.json\"]\n",
        "\n",
        "  if os.path.exists(src_path):\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "  if is_metadata:\n",
        "    parent_meta_path = os.path.dirname(src_path)\n",
        "\n",
        "    for filename in os.listdir(parent_meta_path):\n",
        "      file_path = os.path.join(parent_meta_path, filename)\n",
        "      if filename in files_to_move:\n",
        "        shutil.move(file_path, dst_path)\n",
        "\n",
        "def upload():\n",
        "  if train_data_path !=\"\":\n",
        "    move(train_data_path, tmp_dataset, False)\n",
        "    zip_file(tmp_dataset)\n",
        "    upload_dataset(dataset_zip, True)\n",
        "    if delete_zip:\n",
        "      os.remove(dataset_zip)\n",
        "    \n",
        "  if logs_path !=\"\":\n",
        "    upload_dataset(logs_path, False)\n",
        "\n",
        "upload()\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "IW-hS9jnmf-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.3. Commit using Git (Alternative)"
      ],
      "metadata": {
        "id": "CKZpg4keWS5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ### 8.3.1. Clone Repository\n",
        "\n",
        "clone_model = True #@param {'type': 'boolean'}\n",
        "clone_dataset = True #@param {'type': 'boolean'}\n",
        "\n",
        "!git lfs install --skip-smudge\n",
        "!export GIT_LFS_SKIP_SMUDGE=1\n",
        "\n",
        "if clone_model:\n",
        "  !git clone https://huggingface.co/{model_repo} /content/{model_name}\n",
        "  \n",
        "if clone_dataset:\n",
        "  !git clone https://huggingface.co/datasets/{datasets_repo} /content/{dataset_name}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "6nBlrOrytO9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ### 8.3.2. Commit using Git \n",
        "%cd {root_dir}\n",
        "\n",
        "#@markdown Tick which repo you want to commit\n",
        "commit_model = True #@param {'type': 'boolean'}\n",
        "commit_dataset = True #@param {'type': 'boolean'}\n",
        "\n",
        "#@markdown Set **git commit identity**\n",
        "email = \"your-email-here\" #@param {'type': 'string'}\n",
        "name = \"your-username-here\" #@param {'type': 'string'}\n",
        "#@markdown Set **commit message**\n",
        "commit_message = \"\" #@param {type :\"string\"}\n",
        "\n",
        "if not commit_message:\n",
        "  commit_message = \"feat: upload \"+project_name+\" lora model and dataset\"\n",
        "\n",
        "!git config --global user.email \"{email}\"\n",
        "!git config --global user.name \"{name}\"\n",
        "\n",
        "def commit(repo_folder, commit_message):\n",
        "  %cd {root_dir}/{repo_folder}\n",
        "  !git lfs install\n",
        "  !huggingface-cli lfs-enable-largefiles .\n",
        "  !git add .\n",
        "  !git commit -m \"{commit_message}\"\n",
        "  !git push\n",
        "\n",
        "commit(model_name, commit_message)\n",
        "commit(dataset_name, commit_message)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "7bJev4PzOFFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Become a supporter!\n",
        "[![ko-fi](https://ko-fi.com/img/githubbutton_sm.svg)](https://ko-fi.com/linaqruf)\n",
        "<a href=\"https://saweria.co/linaqruf\"><img alt=\"Saweria\" src=\"https://img.shields.io/badge/Saweria-7B3F00?style=for-the-badge&logo=ko-fi&logoColor=white\"/></a>\n"
      ],
      "metadata": {
        "id": "c9Z5B8_SBwus"
      }
    }
  ]
}
